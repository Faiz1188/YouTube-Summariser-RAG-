# ğŸ¥ YouTube RAG Q&A Application

A **Retrieval-Augmented Generation (RAG)** based application that allows you to **ask questions from any caption-enabled YouTube video**. The app downloads subtitles, converts them into embeddings, stores them in a vector database, and uses an LLM to answer questions **strictly from the video content**.

---

## ğŸš€ Features

* ğŸ“Œ Works with **YouTube captions (auto/manual)**
* ğŸ” Uses **semantic search** with vector embeddings
* ğŸ§  Answers using **RAG (no hallucination)**
* ğŸ’¾ Persistent **Chroma vector database** per video
* âš¡ Fast inference with **Groq (LLaMA 3.1)**
* ğŸ›ï¸ Clean **Streamlit UI** with source inspection

---

## ğŸ§  Architecture Overview

```
YouTube URL
   â†“
Extract Video ID
   â†“
Download Captions (yt-dlp)
   â†“
Clean & Chunk Text
   â†“
Embeddings (HuggingFace)
   â†“
Chroma Vector DB (per video)
   â†“
Retriever (Top-K chunks)
   â†“
LLM (Groq) + RAG Prompt
   â†“
Final Answer
```

---

## ğŸ› ï¸ Tech Stack

* **Python 3.10+**
* **LangChain**
* **yt-dlp** â€“ caption extraction
* **ChromaDB** â€“ vector storage
* **HuggingFace Embeddings** â€“ `all-MiniLM-L6-v2`
* **Groq LLM** â€“ `llama-3.1-8b-instant`
* **Streamlit** â€“ UI

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ rag_pipeline.py        # Core RAG logic (caption â†’ embeddings â†’ answer)
â”œâ”€â”€ app.py                 # Streamlit frontend
â”œâ”€â”€ vector_db/             # Persistent Chroma DB (auto-created)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

### 2ï¸âƒ£ Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate   # Mac/Linux
# .venv\\Scripts\\activate  # Windows
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

Create a `.env` file or export variables:

```bash
export GROQ_API_KEY="your_groq_api_key"
```

---

## â–¶ï¸ Run the App

```bash
streamlit run app.py
```

Open browser at: **[http://localhost:8501](http://localhost:8501)**

---

## ğŸ§ª How It Works (Step-by-Step)

1. User enters **YouTube URL**
2. Video ID is extracted
3. Captions are downloaded using **yt-dlp**
4. Captions are cleaned (timestamps & noise removed)
5. Text is split into overlapping chunks
6. Chunks are converted into embeddings
7. Embeddings are stored in **ChromaDB**
8. On question:

   * Relevant chunks are retrieved
   * Context is injected into RAG prompt
   * LLM answers **only from context**

---

## ğŸ“Œ Important Notes

* â— Works **only if captions are available**
* âŒ Videos without subtitles will throw a warning
* ğŸ’¾ Each video has its **own vector database**
* ğŸ” Reusing same video avoids re-embedding

---

## ğŸ§¾ Example Prompt

> **Question:** What is self-attention?

The system searches captions, retrieves relevant parts, and answers **strictly from the video explanation**.

<img width="1680" height="1050" alt="Screenshot 2026-01-30 at 12 26 17â€¯AM" src="https://github.com/user-attachments/assets/875aa410-2b4f-473a-9b06-7e4703859754" />



---

## ğŸ“š Future Improvements

* ğŸ”¹ Multi-language subtitle support
* ğŸ”¹ Timestamp-based source highlighting
* ğŸ”¹ Chat history (conversational RAG)
* ğŸ”¹ PDF / Blog export of answers
* ğŸ”¹ Docker deployment

---

## ğŸ‘¨â€ğŸ’» Author

**Faiz Ahmad**
Generative AI & RAG Developer

---

## â­ If you like this project

Give it a â­ on GitHub and feel free to contribute!

Happy Learning ğŸš€
